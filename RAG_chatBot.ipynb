{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbe6PwWbOnVx"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-huggingface chromadb pypdf sentence-transformers faiss-cpu transformers\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load and process documents\n",
        "loader = PyPDFLoader(\"/content/Nimeth_Log_report_3.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create embeddings and vector store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Load local model instead of using API\n",
        "model_name = \"google/flan-t5-small\"  # Smaller but works reliably\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create retrieval chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "def ask_question(question):\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"query\": question})\n",
        "        print(\"Answer:\", result[\"result\"])\n",
        "        print(\"\\nSources:\")\n",
        "        for doc in result[\"source_documents\"]:\n",
        "            print(doc.metadata[\"source\"], \"- Page\", doc.metadata.get(\"page\", \"N/A\"))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Test it\n",
        "ask_question(\"What happened on April\")"
      ]
    }
  ]
}